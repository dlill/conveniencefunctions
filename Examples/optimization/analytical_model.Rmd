---
title: "Untitled"
author: "Daniel Lill"
date: "7 July 2018"
output: html_document
---

```{r setup, include=FALSE}
library(conveniencefunctions)
library(dMod)
```



# Goal

Testing whether it is beneficial to add the analytic parts to the hesse-matrix

# Cases to test
1. completely linear y_i=a_i*x_i+b_i + N(0,s)
    * Here I already know that a Gauß-Newton step directly leads to the optimum
2. linear model, log_pars: y_i=exp(a_i)*x_i+exp(b_i) + N(0,s)
    * Here the path is warped. But this cannot be solved by a better Hessian, since in the non-linear case the log-likelihood is not quadratic.
    * Also, in the file "hierarchicaloptimization_without_logpars.R" I found that the second order terms in the Hessian make the optimization even worse
    * It would be possible to include a way to analytically solve for the optimum, but this includes a large overhead in programming
    * My current recommendation would be to parameterize the linear parts not with log-pars but with linear pars and optimize with borders
3. model which depends on other parameters as well y_i=a_i*x_i(p)+b_i + N(0,s): How does the error propagate when p != p_0?
    * This case is rather interesting, since the assumption of a gaußian error distribution doesn't hold anymore and then, the estimate is, of course biased.
    * The question is: Can one say anything about the bias?
        * Probably no

# 3. Model type three

## Simple test-case model

y    = a * x(z,p) + b + N(0, s)
x(z,p) = forcingsSymb("Signal")

p, in a Bayesian framework, has a distribution, let's make it simple and say that the distribution is a normal distribution p ~ N(p_0, s_p)

Three steps now:
Data y is generated with p_0, a_0, b_0 and error

Next, one tries to estimate the parameters from this data. Assume that there is an estimate for p already which is not too far away from the truth. Then, from gaußian error propagation

sy = sqrt(a^2 * sx^2 + s^2)
**i.e. y = a*x(z,p0) + b + N(0,sy) **

However, two remarks here
1. What is sx? Basically this must come from parameter distributions p, and then you could just take the sample variance. However, you don't know anything about the distribution. Usually, it isn't even gaussian.
2. This would only be true for many realizations of the parameters p (and the realizations have to be from the correct distribution). However, usually you have only one realization, namely the one where you start fitting from



```{r}
forcingsSymb("Signal") %>% funC0() %>% 
{ fn <- .
  pars <- fn %>% attr("eq") %>% getSymbols() %>% are_names_of(1) %>% as.list() 
  pars$time <- seq(0,10,0.1)
  do.call(fn,pars)} %>% plot
```

